# -*- coding: utf-8 -*-
"""employeeburnoutprediction_AP22110010174.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TFK6gK4iVZO7ahCs6Hk2FpH3FC1pAhsB

> Add blockquote


EMPLOYEE BURNOUT PREDICTION


This project uses linear regression,SVM(linear kernel),random forest regressor anf decision tree regressor and proves that linear regression is best.
This project allows us to identify those employees at risk of burnout.

> Add blockquote

IMPORTING NECESSARY LIBRARIES
"""

# Commented out IPython magic to ensure Python compatibility.
#Importing necessary libraries
import numpy as np
import pandas as  pd
from sklearn.model_selection import train_test_split
from dateutil.relativedelta import relativedelta
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from pprint import pprint
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor

from sklearn.linear_model import LinearRegression
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score
import pickle
import os



"""LOADING DATASET"""

data=pd.read_excel("/content/employee_burnout_analysis-AI.xlsx")

"""DATA OVERVIEW"""

data.head()

data.tail()

data.describe()

data.shape

data.size

data.columns.tolist()

data.nunique()

data.info()

data.isnull().sum()

data.isnull().sum().values.sum()

"""EXPLORATORY DATA ANALYSIS

There are NaN values on our target ("Burn Rate") and also in Resource Allocation and Mental Fatigue Score columns. As we are going to perform supervised linear regression, our target variable is needed to do so. Therefore, this 1124 rows with NaN values must be dropped off of our dataframe.




"""

data.corr(numeric_only=True)['Burn Rate'][:-1]

"""These two variables are strongly correlated"""

sns.pairplot(data)
plt.show()

"""drop off all obs with nan values"""

data = data.dropna()

data.shape

"""Analyzing what type of data is each variable.


"""

data.dtypes

"""employee ids should be dropped since it doesn't provide any useful information."""

data=data.drop('Employee ID',axis =1 )

"""Checking the correlation of date of joining with target variable"""

print(f"Min date {data['Date of Joining'].min()}")
print(f"Max date {data['Date of Joining'].max()}")
data_month = data.copy()

data_month["Date of Joining"] = data_month['Date of Joining'].astype("datetime64[ns]")  # Specify time unit as nanoseconds
data_month["Date of Joining"].groupby(data_month['Date of Joining'].dt.month).count().plot(kind="bar", xlabel='Month', ylabel="Hired employees")

"""The date of joining is uniform distributed with values between 2008-01-01 and 2008-12-31. So in order to create a new feature which represents the labor seniority, we could create a variable with de days worked"""

data_2008 = pd.to_datetime(["2008-01-01"]*len(data))
data["Days"] = data['Date of Joining'].astype("datetime64[ns]").sub(data_2008).dt.days
data.Days

numeric_data=data.select_dtypes(include=['number'])
correlation=numeric_data.corr()['Burn Rate']
print(correlation)

data.corr(numeric_only=True)['Burn Rate'][:]

data = data.drop(['Date of Joining','Days'], axis = 1)

data.head()

"""We are analysing categorical values now"""

cat_columns = data.select_dtypes(object).columns
fig, ax = plt.subplots(nrows=1, ncols=len(cat_columns), sharey=True, figsize=(10, 5))
for i, c in enumerate(cat_columns):
    sns.countplot(x=c, data=data, ax=ax[i])
plt.show()

"""The number of observations of each category on each variable is equally distributed, except to the Company_Type where the number of service jobs its almost twice that of product ones."""

data.columns

"""one hot encoding for categorical values"""

if all(col in data.columns for col in ['Company Type','WFH Setup Available','Gender']):
  data=pd.get_dummies(data,columns=['Company Type','WFH Setup Available','Gender'],drop_first=True)
  data.head()
  encoded_columns=data.columns
else:
  print("Error: One or more of the specified columns are not present in the DataFrame.")
  print(data.columns)

"""preprocessing"""

# Split df into X and y
y = data['Burn Rate']
X = data.drop('Burn Rate', axis=1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)
#scale x
scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)
X_test_scaled = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)

X_train

y_train

"""MODEL BUILDING
LINEAR REGRESSION
"""

linear_regression_model = LinearRegression()

# Train the model
linear_regression_model.fit(X_train, y_train)

print("Linear Regression Model Performance Metrics:\n")
# Make predictions on the test set
y_pred = linear_regression_model.predict(X_test)

# Calculate mean squared error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Calculate root mean squared error
rmse = mean_squared_error(y_test, y_pred, squared=False)
print("Root Mean Squared Error:", rmse)

# Calculate mean absolute error
mae = mean_absolute_error(y_test, y_pred)
print("Mean Absolute Error:", mae)

# Calculate R-squared score
r2 = r2_score(y_test, y_pred)
print("R-squared Score:", r2)

feature_names = X.columns.tolist()
feature_names

"""SUPPORT VECTOR MACHINE(LINEAR KERNEL)"""

y = data['Burn Rate']
X = data.drop('Burn Rate', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

SVMLinear = LinearSVR(dual=True, max_iter=10000)

# Fit the model
SVMLinear.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_test_pred = SVMLinear.predict(X_test_scaled)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_test_pred)
rmse = mean_squared_error(y_test, y_test_pred, squared=False)
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)

print("Linear SVR Model Performance Metrics:\n")
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared Score:", r2)



"""RANDOM FOREST REGRESSOR"""

y = data['Burn Rate']
X = data.drop('Burn Rate', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_test_pred = rf_regressor.predict(X_test_scaled)

# Calculate performance metrics
mse = mean_squared_error(y_test, y_test_pred)
rmse = mean_squared_error(y_test, y_test_pred, squared=False)
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)

print("Random Forest Regressor Model Performance Metrics:\n")
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared Score:", r2)

"""DECISION TREE REGRESSOR"""

y = data['Burn Rate']
X = data.drop('Burn Rate', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)

scaler = StandardScaler()
scaler.fit(X_train)

X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Decision Tree Regressor
dt_regressor = DecisionTreeRegressor(random_state=42)  # Instantiate the regressor
dt_regressor.fit(X_train_scaled, y_train)  # Fit the model

# Make predictions on the test set
y_test_pred = dt_regressor.predict(X_test_scaled)  # Predict

# Calculate performance metrics
mse = mean_squared_error(y_test, y_test_pred)
rmse = mean_squared_error(y_test, y_test_pred, squared=False)
mae = mean_absolute_error(y_test, y_test_pred)
r2 = r2_score(y_test, y_test_pred)

print("Decision Tree Regressor Model Performance Metrics:\n")
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("Mean Absolute Error:", mae)
print("R-squared Score:", r2)